{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66c5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import textblob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51825ea1",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0f588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Huh, anyway check out this you[tube] channel: ...\n",
      "1    Hey guys check out my new channel and our firs...\n",
      "2               just for test I have to say murdev.com\n",
      "3     me shaking my sexy ass on my channel enjoy ^_^ ﻿\n",
      "4              watch?v=vtaRGgvGtWQ   Check this out .﻿\n",
      "Name: CONTENT, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../DATA/comments.csv\")\n",
    "print(df[\"CONTENT\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02054f3e",
   "metadata": {},
   "source": [
    "ESSENTIAL DOWNLOADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c600fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\swoye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\swoye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swoye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\swoye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swoye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc6db6",
   "metadata": {},
   "source": [
    "BASIC PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961fbc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicPre(text):\n",
    "    text = text.lower() # lower text\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # only letters in text\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) # remove urls\n",
    "    text = str(TextBlob(text).correct()) # correct the spelling\n",
    "    tokens = word_tokenize(text) # tokenize text\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871942a",
   "metadata": {},
   "source": [
    "STOPWORD REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faef8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordRemoval(tokens):\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc4c60",
   "metadata": {},
   "source": [
    "STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510888a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stemming(tokens):\n",
    "#     tokens = [stemmer.stem(token) for token in tokens]\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85971a",
   "metadata": {},
   "source": [
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c700ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(tokens):\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9f6517",
   "metadata": {},
   "source": [
    "PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43bda790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningPipeline(text):\n",
    "    tokens = basicPre(text)\n",
    "    tokens = stopwordRemoval(tokens)\n",
    "    tokens = lemmatization(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7200a8",
   "metadata": {},
   "source": [
    "PREPROCESSED COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preprocessedComments\"] = df[\"CONTENT\"].apply(cleaningPipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
